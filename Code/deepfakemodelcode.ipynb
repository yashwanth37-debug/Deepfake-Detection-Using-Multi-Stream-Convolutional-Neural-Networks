{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7615428,"sourceType":"datasetVersion","datasetId":4434986},{"sourceId":11471738,"sourceType":"datasetVersion","datasetId":7189290},{"sourceId":11472067,"sourceType":"datasetVersion","datasetId":7189512},{"sourceId":11478585,"sourceType":"datasetVersion","datasetId":7194259}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install facenet-pytorch timm albumentations --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T19:13:06.338497Z","iopub.execute_input":"2025-04-19T19:13:06.338853Z","iopub.status.idle":"2025-04-19T19:15:38.572089Z","shell.execute_reply.started":"2025-04-19T19:13:06.338818Z","shell.execute_reply":"2025-04-19T19:15:38.571111Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.6/755.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\ntorchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.2.2 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install --upgrade --force-reinstall Pillow\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T19:15:38.574254Z","iopub.execute_input":"2025-04-19T19:15:38.575041Z","iopub.status.idle":"2025-04-19T19:15:46.293718Z","shell.execute_reply.started":"2025-04-19T19:15:38.574977Z","shell.execute_reply":"2025-04-19T19:15:46.293048Z"}},"outputs":[{"name":"stdout","text":"Collecting Pillow\n  Downloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\nDownloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (4.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: Pillow\n  Attempting uninstall: Pillow\n    Found existing installation: pillow 10.2.0\n    Uninstalling pillow-10.2.0:\n      Successfully uninstalled pillow-10.2.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfacenet-pytorch 2.6.0 requires Pillow<10.3.0,>=10.2.0, but you have pillow 11.2.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed Pillow-11.2.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport torch\nimport timm\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom facenet_pytorch import MTCNN\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import classification_report, roc_auc_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T19:16:49.646165Z","iopub.execute_input":"2025-04-19T19:16:49.646431Z","iopub.status.idle":"2025-04-19T19:16:59.109769Z","shell.execute_reply.started":"2025-04-19T19:16:49.646409Z","shell.execute_reply":"2025-04-19T19:16:59.109161Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"Using device:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T19:17:14.515555Z","iopub.execute_input":"2025-04-19T19:17:14.516221Z","iopub.status.idle":"2025-04-19T19:17:14.600678Z","shell.execute_reply.started":"2025-04-19T19:17:14.516193Z","shell.execute_reply":"2025-04-19T19:17:14.600065Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"REAL_PATH = \"/kaggle/input/original-ds/Original\"\nFAKE_PATH = \"/kaggle/input/fake-800/Fake800\"\nNUM_FRAMES_PER_VIDEO = 5\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T19:17:18.772762Z","iopub.execute_input":"2025-04-19T19:17:18.773506Z","iopub.status.idle":"2025-04-19T19:17:18.777341Z","shell.execute_reply.started":"2025-04-19T19:17:18.773475Z","shell.execute_reply":"2025-04-19T19:17:18.776522Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"mtcnn = MTCNN(image_size=224, margin=0, device=device)\n\ndef extract_faces(video_dir, label, num_frames=5):\n    face_list = []\n    label_list = []\n    \n    for video_file in tqdm(os.listdir(video_dir), desc=f\"Processing {label} videos\"):\n        video_path = os.path.join(video_dir, video_file)\n        cap = cv2.VideoCapture(video_path)\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        frame_idxs = np.linspace(0, total_frames-1, num_frames, dtype=int)\n\n        for idx in frame_idxs:\n            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n            success, frame = cap.read()\n            if not success:\n                continue\n\n            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            face = mtcnn(rgb)\n\n            if face is not None:\n                face_list.append(face)\n                label_list.append(label)\n        cap.release()\n\n    return face_list, label_list\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T19:17:33.592322Z","iopub.execute_input":"2025-04-19T19:17:33.592586Z","iopub.status.idle":"2025-04-19T19:17:33.617466Z","shell.execute_reply.started":"2025-04-19T19:17:33.592565Z","shell.execute_reply":"2025-04-19T19:17:33.616729Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"real_faces, real_labels = extract_faces(REAL_PATH, 0, NUM_FRAMES_PER_VIDEO)\nfake_faces, fake_labels = extract_faces(FAKE_PATH, 1, NUM_FRAMES_PER_VIDEO)\n\nX_faces = real_faces + fake_faces\ny_labels = real_labels + fake_labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T19:26:02.088899Z","iopub.execute_input":"2025-04-19T19:26:02.089587Z","iopub.status.idle":"2025-04-19T20:13:55.274331Z","shell.execute_reply.started":"2025-04-19T19:26:02.089564Z","shell.execute_reply":"2025-04-19T20:13:55.273715Z"}},"outputs":[{"name":"stderr","text":"Processing 0 videos: 100%|██████████| 368/368 [14:25<00:00,  2.35s/it]\nProcessing 1 videos: 100%|██████████| 804/804 [33:27<00:00,  2.50s/it]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:14:06.243708Z","iopub.execute_input":"2025-04-19T20:14:06.244278Z","iopub.status.idle":"2025-04-19T20:14:06.268550Z","shell.execute_reply.started":"2025-04-19T20:14:06.244254Z","shell.execute_reply":"2025-04-19T20:14:06.267813Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def prepare_rgb(face_tensor):\n    return face_tensor  # already in [3, 224, 224], normalized to [0, 1]\n\ndef prepare_fft(image_tensor):\n    image_np = image_tensor.numpy().transpose(1, 2, 0)\n    gray = cv2.cvtColor((image_np * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n    f = np.fft.fft2(gray)\n    fshift = np.fft.fftshift(f)\n    magnitude_spectrum = 20 * np.log(np.abs(fshift) + 1)\n    magnitude_spectrum = cv2.resize(magnitude_spectrum, (image_np.shape[1], image_np.shape[0]))\n    magnitude_spectrum = (magnitude_spectrum - magnitude_spectrum.min()) / (magnitude_spectrum.max() - magnitude_spectrum.min() + 1e-8)\n    magnitude_tensor = torch.tensor(magnitude_spectrum, dtype=torch.float32).unsqueeze(0)\n    return magnitude_tensor\n\ndef prepare_motion(current_tensor, previous_tensor):\n    motion = current_tensor - previous_tensor\n    motion = torch.abs(motion)\n    return motion","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:14:08.605485Z","iopub.execute_input":"2025-04-19T20:14:08.605748Z","iopub.status.idle":"2025-04-19T20:14:08.611795Z","shell.execute_reply.started":"2025-04-19T20:14:08.605728Z","shell.execute_reply":"2025-04-19T20:14:08.610933Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass MultiStreamDeepfakeDataset(Dataset):\n    def __init__(self, rgb_faces, fft_images, motion_images, labels, transform=None):\n        self.rgb_faces = rgb_faces\n        self.fft_images = fft_images\n        self.motion_images = motion_images\n        self.labels = labels\n        self.transform = transform  # Optional\n\n    def __len__(self):\n        return len(self.rgb_faces)\n\n    def __getitem__(self, idx):\n        rgb = self.rgb_faces[idx]\n        fft = self.fft_images[idx]\n        motion = self.motion_images[idx]\n        label = self.labels[idx]\n\n        # Apply transforms if needed\n        if self.transform:\n            rgb = self.transform(rgb)\n            fft = self.transform(fft)\n            motion = self.transform(motion)\n\n        return rgb, fft, motion, torch.tensor(label, dtype=torch.float32)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:14:15.823567Z","iopub.execute_input":"2025-04-19T20:14:15.823866Z","iopub.status.idle":"2025-04-19T20:14:15.829616Z","shell.execute_reply.started":"2025-04-19T20:14:15.823845Z","shell.execute_reply":"2025-04-19T20:14:15.828798Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import torch.nn.functional as F\n\nX_faces_tensor = [\n    F.interpolate(face.float().unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n    for face in X_faces\n]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:14:15.830534Z","iopub.execute_input":"2025-04-19T20:14:15.830764Z","iopub.status.idle":"2025-04-19T20:14:20.510503Z","shell.execute_reply.started":"2025-04-19T20:14:15.830743Z","shell.execute_reply":"2025-04-19T20:14:20.509724Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class CBAM(nn.Module):\n    def __init__(self, channels, reduction_ratio=16, kernel_size=7):\n        super(CBAM, self).__init__()\n        self.channel_attention = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(channels, channels // reduction_ratio, 1, bias=False),\n            nn.ReLU(),\n            nn.Conv2d(channels // reduction_ratio, channels, 1, bias=False),\n            nn.Sigmoid()\n        )\n        self.spatial_attention = nn.Sequential(\n            nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        ca = self.channel_attention(x)\n        x = x * ca\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        sa = self.spatial_attention(torch.cat([avg_out, max_out], dim=1))\n        x = x * sa\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:14:20.511932Z","iopub.execute_input":"2025-04-19T20:14:20.512224Z","iopub.status.idle":"2025-04-19T20:14:20.522983Z","shell.execute_reply.started":"2025-04-19T20:14:20.512199Z","shell.execute_reply":"2025-04-19T20:14:20.521955Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class MultiStreamEfficientNet(nn.Module):\n    def __init__(self):\n        super(MultiStreamEfficientNet, self).__init__()\n        self.rgb_model = models.efficientnet_b0(pretrained=True).features\n        self.fft_model = models.efficientnet_b0(pretrained=True).features\n        self.motion_model = models.efficientnet_b0(pretrained=True).features\n\n        self.rgb_attention = CBAM(1280)\n        self.fft_attention = CBAM(1280)\n        self.motion_attention = CBAM(1280)\n\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=1280, nhead=8, batch_first=True),\n            num_layers=2\n        )\n\n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Linear(1280 * 3, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, rgb, fft, motion):\n        rgb_feat = self.rgb_model(rgb)\n        fft_feat = self.fft_model(fft)\n        motion_feat = self.motion_model(motion)\n\n        rgb_feat = self.rgb_attention(rgb_feat)\n        fft_feat = self.fft_attention(fft_feat)\n        motion_feat = self.motion_attention(motion_feat)\n\n        b, c, h, w = rgb_feat.size()\n        rgb_feat = rgb_feat.view(b, c, -1).permute(0, 2, 1)\n        fft_feat = fft_feat.view(b, c, -1).permute(0, 2, 1)\n        motion_feat = motion_feat.view(b, c, -1).permute(0, 2, 1)\n\n        combined = torch.cat([rgb_feat, fft_feat, motion_feat], dim=1)\n        fused = self.transformer(combined)\n        fused = fused.permute(0, 2, 1).view(b, 1280 * 3, h, w)\n\n        output = self.classifier(fused)\n        return output.squeeze(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:14:21.972490Z","iopub.execute_input":"2025-04-19T20:14:21.972768Z","iopub.status.idle":"2025-04-19T20:14:21.982395Z","shell.execute_reply.started":"2025-04-19T20:14:21.972750Z","shell.execute_reply":"2025-04-19T20:14:21.981540Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def train_one_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    for batch in tqdm(dataloader, desc=\"Training\"):\n        rgb = batch['rgb'].to(device)\n        fft = batch['fft'].to(device)\n        motion = batch['motion'].to(device)\n        labels = batch['label'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(rgb, fft, motion)\n        loss = criterion(outputs.unsqueeze(1), labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    return running_loss / len(dataloader)\n\ndef validate(model, dataloader, device):\n    model.eval()\n    preds, targets = [], []\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Validation\"):\n            rgb = batch['rgb'].to(device)\n            fft = batch['fft'].to(device)\n            motion = batch['motion'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(rgb, fft, motion)\n            preds.extend(outputs.cpu().numpy().squeeze().tolist())\n            targets.extend(labels.cpu().numpy().squeeze().tolist())\n\n    preds = np.array(preds) > 0.5\n    accuracy = accuracy_score(targets, preds)\n    precision = precision_score(targets, preds)\n    recall = recall_score(targets, preds)\n    f1 = f1_score(targets, preds)\n    auc = roc_auc_score(targets, preds)\n    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"auc\": auc}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:14:26.980893Z","iopub.execute_input":"2025-04-19T20:14:26.981540Z","iopub.status.idle":"2025-04-19T20:14:26.989045Z","shell.execute_reply.started":"2025-04-19T20:14:26.981513Z","shell.execute_reply":"2025-04-19T20:14:26.988326Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Utility functions\ndef prepare_fft(tensor):\n    gray = tensor.mean(dim=0, keepdim=True)\n    fft = torch.fft.fft2(gray)\n    fft_mag = torch.abs(fft)\n    fft_norm = (fft_mag - fft_mag.min()) / (fft_mag.max() - fft_mag.min() + 1e-8)\n    fft_img = fft_norm.expand(3, -1, -1)\n    return fft_img\n\ndef prepare_motion(current_tensor, prev_tensor):\n    return torch.abs(current_tensor - prev_tensor)\n\n# Input: X_faces_tensor (list of torch tensors) and y_labels (list of 0/1)\nfft_tensors = [prepare_fft(face) for face in X_faces_tensor]\n\nmotion_tensors = [\n    prepare_motion(X_faces_tensor[i], X_faces_tensor[i-1]) if i > 0 else torch.zeros_like(X_faces_tensor[i])\n    for i in range(len(X_faces_tensor))\n]\n\n# Create dataset\ndataset = MultiStreamDeepfakeDataset(X_faces_tensor, fft_tensors, motion_tensors, y_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:14:30.053127Z","iopub.execute_input":"2025-04-19T20:14:30.053909Z","iopub.status.idle":"2025-04-19T20:14:38.969970Z","shell.execute_reply.started":"2025-04-19T20:14:30.053878Z","shell.execute_reply":"2025-04-19T20:14:38.968990Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\n\ntrain_set, val_set = train_test_split(dataset, test_size=0.2, random_state=42)\n\ntrain_loader = DataLoader(train_set, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=16, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:14:39.371357Z","iopub.execute_input":"2025-04-19T20:14:39.371630Z","iopub.status.idle":"2025-04-19T20:14:39.418555Z","shell.execute_reply.started":"2025-04-19T20:14:39.371610Z","shell.execute_reply":"2025-04-19T20:14:39.417692Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models\n\n# CBAM Attention Block (simplified)\nclass CBAM(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super(CBAM, self).__init__()\n        self.channel_attention = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(channels, channels // reduction, 1),\n            nn.ReLU(),\n            nn.Conv2d(channels // reduction, channels, 1),\n            nn.Sigmoid()\n        )\n        self.spatial_attention = nn.Sequential(\n            nn.Conv2d(2, 1, kernel_size=7, padding=3),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        # Channel attention\n        ca = self.channel_attention(x)\n        x = x * ca\n\n        # Spatial attention\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        sa = torch.cat([avg_out, max_out], dim=1)\n        sa = self.spatial_attention(sa)\n        x = x * sa\n        return x\n\n# Stream module (EfficientNet + CBAM)\nclass StreamNet(nn.Module):\n    def __init__(self):\n        super(StreamNet, self).__init__()\n        base_model = models.efficientnet_b0(pretrained=True)\n        self.features = base_model.features\n        self.cbam = CBAM(1280)\n        self.pool = nn.AdaptiveAvgPool2d(1)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.cbam(x)\n        x = self.pool(x).view(x.size(0), -1)  # Flatten\n        return x\n\n# MultiStream Model\nclass MultiStreamModel(nn.Module):\n    def __init__(self):\n        super(MultiStreamModel, self).__init__()\n        self.rgb_stream = StreamNet()\n        self.fft_stream = StreamNet()\n        self.motion_stream = StreamNet()\n\n        self.classifier = nn.Sequential(\n            nn.Linear(1280 * 3, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, rgb, fft, motion):\n        rgb_feat = self.rgb_stream(rgb)\n        fft_feat = self.fft_stream(fft)\n        motion_feat = self.motion_stream(motion)\n\n        combined = torch.cat([rgb_feat, fft_feat, motion_feat], dim=1)\n        return self.classifier(combined)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:14:44.392445Z","iopub.execute_input":"2025-04-19T20:14:44.392797Z","iopub.status.idle":"2025-04-19T20:14:44.402632Z","shell.execute_reply.started":"2025-04-19T20:14:44.392771Z","shell.execute_reply":"2025-04-19T20:14:44.401955Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = MultiStreamModel().to(device)\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:14:53.413601Z","iopub.execute_input":"2025-04-19T20:14:53.413887Z","iopub.status.idle":"2025-04-19T20:14:54.168243Z","shell.execute_reply.started":"2025-04-19T20:14:53.413866Z","shell.execute_reply":"2025-04-19T20:14:54.167642Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n100%|██████████| 20.5M/20.5M [00:00<00:00, 119MB/s] \n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:14:55.971314Z","iopub.execute_input":"2025-04-19T20:14:55.971568Z","iopub.status.idle":"2025-04-19T20:14:59.303635Z","shell.execute_reply.started":"2025-04-19T20:14:55.971550Z","shell.execute_reply":"2025-04-19T20:14:59.302715Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from tqdm import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:15:07.229221Z","iopub.execute_input":"2025-04-19T20:15:07.229476Z","iopub.status.idle":"2025-04-19T20:15:07.233150Z","shell.execute_reply.started":"2025-04-19T20:15:07.229457Z","shell.execute_reply":"2025-04-19T20:15:07.232450Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"from tqdm import tqdm\n\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n\n    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n    train_loader_tqdm = tqdm(train_loader, desc=\"Training\", leave=False)\n\n    for rgb, fft, motion, labels in train_loader_tqdm:\n        rgb, fft, motion, labels = rgb.to(device), fft.to(device), motion.to(device), labels.to(device).float()\n\n        optimizer.zero_grad()\n        outputs = model(rgb, fft, motion).squeeze()\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        train_loader_tqdm.set_postfix(loss=loss.item())\n\n    model.eval()\n    val_loss = 0.0\n    correct, total = 0, 0\n\n    val_loader_tqdm = tqdm(val_loader, desc=\"Validating\", leave=False)\n\n    with torch.no_grad():\n        for rgb, fft, motion, labels in val_loader_tqdm:\n            rgb, fft, motion, labels = rgb.to(device), fft.to(device), motion.to(device), labels.to(device).float()\n\n            outputs = model(rgb, fft, motion).squeeze()\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            preds = (outputs > 0.5).long()\n            correct += (preds == labels.long()).sum().item()\n            total += labels.size(0)\n\n    acc = correct / total\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:15:08.458312Z","iopub.execute_input":"2025-04-19T20:15:08.458838Z","iopub.status.idle":"2025-04-19T20:28:21.822924Z","shell.execute_reply.started":"2025-04-19T20:15:08.458819Z","shell.execute_reply":"2025-04-19T20:28:21.822100Z"}},"outputs":[{"name":"stdout","text":"\nEpoch [1/10]\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10], Train Loss: 85.9048, Val Loss: 13.2706, Val Acc: 0.9200\n\nEpoch [2/10]\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch [2/10], Train Loss: 30.1417, Val Loss: 12.0706, Val Acc: 0.9396\n\nEpoch [3/10]\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch [3/10], Train Loss: 20.7694, Val Loss: 13.1820, Val Acc: 0.9422\n\nEpoch [4/10]\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [4/10], Train Loss: 12.5793, Val Loss: 11.4029, Val Acc: 0.9538\n\nEpoch [5/10]\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [5/10], Train Loss: 9.3481, Val Loss: 12.3093, Val Acc: 0.9529\n\nEpoch [6/10]\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [6/10], Train Loss: 8.6310, Val Loss: 16.6622, Val Acc: 0.9387\n\nEpoch [7/10]\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [7/10], Train Loss: 6.9385, Val Loss: 13.2371, Val Acc: 0.9556\n\nEpoch [8/10]\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [8/10], Train Loss: 6.9509, Val Loss: 16.0349, Val Acc: 0.9484\n\nEpoch [9/10]\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [9/10], Train Loss: 8.5136, Val Loss: 10.7664, Val Acc: 0.9582\n\nEpoch [10/10]\n","output_type":"stream"},{"name":"stderr","text":"                                                                          ","output_type":"stream"},{"name":"stdout","text":"Epoch [10/10], Train Loss: 4.2649, Val Loss: 13.3352, Val Acc: 0.9547\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n\nall_preds, all_labels = [], []\n\nmodel.eval()\nwith torch.no_grad():\n    for rgb, fft, motion, labels in val_loader:\n        rgb, fft, motion = rgb.to(device), fft.to(device), motion.to(device)\n        labels = labels.to(device).long()\n\n        outputs = model(rgb, fft, motion).squeeze().cpu()\n        preds = (outputs > 0.5).long()\n        \n\n        all_preds.extend(preds.tolist())\n        all_labels.extend(labels.cpu().tolist())\n\n# Classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(all_labels, all_preds, target_names=[\"Real\", \"Fake\"]))\n\n# Confusion matrix\ncm = confusion_matrix(all_labels, all_preds)\nprint(\"Confusion Matrix:\\n\", cm)\n\n# AUC-ROC Score\nprob_outputs = [float(o) for o in outputs]  # Last batch only, optional improvement below\nroc_auc = roc_auc_score(all_labels, all_preds)\nprint(\"ROC AUC Score:\", roc_auc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T21:29:11.423359Z","iopub.execute_input":"2025-04-19T21:29:11.424012Z","iopub.status.idle":"2025-04-19T21:29:16.450716Z","shell.execute_reply.started":"2025-04-19T21:29:11.423991Z","shell.execute_reply":"2025-04-19T21:29:16.449976Z"}},"outputs":[{"name":"stdout","text":"\nClassification Report:\n              precision    recall  f1-score   support\n\n        Real       0.96      0.90      0.93       370\n        Fake       0.95      0.98      0.97       755\n\n    accuracy                           0.95      1125\n   macro avg       0.96      0.94      0.95      1125\nweighted avg       0.95      0.95      0.95      1125\n\nConfusion Matrix:\n [[334  36]\n [ 15 740]]\nROC AUC Score: 0.9414175765169143\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"# After training your model\ntorch.save(model, \"deepfake_detector_full.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:28:48.080532Z","iopub.execute_input":"2025-04-19T20:28:48.081102Z","iopub.status.idle":"2025-04-19T20:28:48.309512Z","shell.execute_reply.started":"2025-04-19T20:28:48.081082Z","shell.execute_reply":"2025-04-19T20:28:48.308921Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import os\nimport cv2\nimport csv\nimport torch\nimport numpy as np\nfrom facenet_pytorch import MTCNN\nfrom torchvision import transforms\nfrom torch.nn.functional import interpolate\nfrom tqdm import tqdm\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmtcnn = MTCNN(image_size=224, margin=0, device=device)\nNUM_FRAMES_PER_VIDEO = 5\n\ndef prepare_fft(tensor):\n    gray = tensor.mean(dim=0, keepdim=True)\n    fft = torch.fft.fft2(gray)\n    fft_mag = torch.abs(fft)\n    fft_norm = (fft_mag - fft_mag.min()) / (fft_mag.max() - fft_mag.min() + 1e-8)\n    return fft_norm.expand(3, -1, -1)\n\ndef prepare_motion(curr, prev):\n    return torch.abs(curr - prev) if prev is not None else torch.zeros_like(curr)\n\ndef predict_on_dataset(model, dataset_path, output_csv=\"predictions.csv\"):\n    model.eval()\n    video_extensions = (\".mp4\", \".avi\", \".mov\")\n\n    with open(output_csv, mode='w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow([\"video_path\", \"label\", \"predicted_label\", \"confidence\"])\n\n        for label_str in [\"real\", \"fake\"]:\n            label = 0 if label_str == \"real\" else 1\n            folder_path = os.path.join(dataset_path, label_str)\n\n            for video_file in tqdm(os.listdir(folder_path), desc=f\"Processing {label_str}\"):\n                if not video_file.lower().endswith(video_extensions):\n                    continue\n\n                video_path = os.path.join(folder_path, video_file)\n                cap = cv2.VideoCapture(video_path)\n                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n                frame_idxs = np.linspace(0, total_frames - 1, NUM_FRAMES_PER_VIDEO, dtype=int)\n\n                rgb_tensors, fft_tensors, motion_tensors = [], [], []\n                prev_tensor = None\n\n                for idx in frame_idxs:\n                    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n                    success, frame = cap.read()\n                    if not success:\n                        continue\n\n                    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                    face = mtcnn(rgb)\n\n                    if face is None:\n                        continue\n\n                    face = interpolate(face.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n                    rgb_tensor = face.to(device)\n\n                    fft_tensor = prepare_fft(rgb_tensor)\n                    motion_tensor = prepare_motion(rgb_tensor, prev_tensor)\n                    prev_tensor = rgb_tensor.clone()\n\n                    rgb_tensors.append(rgb_tensor)\n                    fft_tensors.append(fft_tensor)\n                    motion_tensors.append(motion_tensor)\n\n                cap.release()\n\n                if not rgb_tensors:\n                    writer.writerow([f\"{label_str}/{video_file}\", label, \"No Face Detected\", \"N/A\"])\n                    continue\n\n                rgb_batch = torch.stack(rgb_tensors)\n                fft_batch = torch.stack(fft_tensors)\n                motion_batch = torch.stack(motion_tensors)\n\n                with torch.no_grad():\n                    output = model(rgb_batch, fft_batch, motion_batch)\n                    pred = torch.sigmoid(output).cpu().numpy().mean()\n                    predicted_label = 1 if pred > 0.5 else 0\n                    writer.writerow([f\"{label_str}/{video_file}\", label, predicted_label, round(pred, 4)])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:47:24.352382Z","iopub.execute_input":"2025-04-19T20:47:24.353138Z","iopub.status.idle":"2025-04-19T20:47:24.381123Z","shell.execute_reply.started":"2025-04-19T20:47:24.353086Z","shell.execute_reply":"2025-04-19T20:47:24.380385Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Example test data paths\nREAL_PATH = \"/kaggle/input/dataset/DSF/real\"\nFAKE_PATH = \"/kaggle/input/dataset/DSF/fake\"\n\n# Select only 3 real and 3 fake videos\nreal_video_files = sorted(os.listdir(REAL_PATH))[:3]\nfake_video_files = sorted(os.listdir(FAKE_PATH))[:3]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:59:45.456076Z","iopub.execute_input":"2025-04-19T20:59:45.456656Z","iopub.status.idle":"2025-04-19T20:59:45.462692Z","shell.execute_reply.started":"2025-04-19T20:59:45.456633Z","shell.execute_reply":"2025-04-19T20:59:45.462060Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"NUM_FRAMES_PER_VIDEO = 5  # or your preferred number\n\ndef extract_faces_from_list(video_dir, video_files, label, num_frames=5):\n    face_list = []\n    label_list = []\n    for video_file in video_files:\n        video_path = os.path.join(video_dir, video_file)\n        cap = cv2.VideoCapture(video_path)\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        frame_idxs = np.linspace(0, total_frames-1, num_frames, dtype=int)\n        for idx in frame_idxs:\n            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n            success, frame = cap.read()\n            if not success:\n                continue\n            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            face = mtcnn(rgb)\n            if face is not None:\n                face_list.append(face)\n                label_list.append(label)\n        cap.release()\n    return face_list, label_list\n\nreal_faces, real_labels = extract_faces_from_list(REAL_PATH, real_video_files, 0, NUM_FRAMES_PER_VIDEO)\nfake_faces, fake_labels = extract_faces_from_list(FAKE_PATH, fake_video_files, 1, NUM_FRAMES_PER_VIDEO)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:59:58.865377Z","iopub.execute_input":"2025-04-19T20:59:58.866079Z","iopub.status.idle":"2025-04-19T21:00:03.782692Z","shell.execute_reply.started":"2025-04-19T20:59:58.866058Z","shell.execute_reply":"2025-04-19T21:00:03.782075Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n\nmodel.eval()\nall_preds, all_labels, all_probs = [], [], []\n\nwith torch.no_grad():\n    for rgb, fft, motion, labels in test_loader:\n        rgb, fft, motion = rgb.to(device), fft.to(device), motion.to(device)\n        outputs = model(rgb, fft, motion).squeeze().cpu()\n        probs = outputs.numpy()\n        preds = (outputs > 0.5).long()\n        all_preds.extend(preds.tolist())\n        all_labels.extend(labels.tolist())\n        all_probs.extend(probs.tolist())\n\n# Print metrics\nprint(\"\\nClassification Report:\")\nprint(classification_report(all_labels, all_preds, target_names=[\"Real\", \"Fake\"]))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(all_labels, all_preds))\nprint(\"ROC AUC Score:\", roc_auc_score(all_labels, all_probs))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:58:56.986778Z","iopub.execute_input":"2025-04-19T20:58:56.987359Z","iopub.status.idle":"2025-04-19T20:58:57.371084Z","shell.execute_reply.started":"2025-04-19T20:58:56.987334Z","shell.execute_reply":"2025-04-19T20:58:57.370324Z"}},"outputs":[{"name":"stdout","text":"\nClassification Report:\n              precision    recall  f1-score   support\n\n        Real       0.50      0.93      0.65        15\n        Fake       0.50      0.07      0.12        15\n\n    accuracy                           0.50        30\n   macro avg       0.50      0.50      0.38        30\nweighted avg       0.50      0.50      0.38        30\n\nConfusion Matrix:\n [[14  1]\n [14  1]]\nROC AUC Score: 0.5733333333333333\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"import torch.nn.functional as F\n\nX_faces = real_faces + fake_faces\ny_labels = real_labels + fake_labels\n\n# Resize faces to 224x224\nX_faces_tensor = [\n    F.interpolate(face.float().unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n    for face in X_faces\n]\n\n# Prepare FFT tensors\nfft_tensors = [prepare_fft(face) for face in X_faces_tensor]\n\n# Prepare motion tensors\nmotion_tensors = [\n    prepare_motion(X_faces_tensor[i], X_faces_tensor[i-1]) if i > 0 else torch.zeros_like(X_faces_tensor[i])\n    for i in range(len(X_faces_tensor))\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T21:00:29.985918Z","iopub.execute_input":"2025-04-19T21:00:29.986664Z","iopub.status.idle":"2025-04-19T21:00:30.037030Z","shell.execute_reply.started":"2025-04-19T21:00:29.986641Z","shell.execute_reply":"2025-04-19T21:00:30.036292Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"class MultiStreamDeepfakeDataset(Dataset):\n    def __init__(self, rgb_faces, fft_images, motion_images, labels, transform=None):\n        self.rgb_faces = rgb_faces\n        self.fft_images = fft_images\n        self.motion_images = motion_images\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.rgb_faces)\n\n    def __getitem__(self, idx):\n        rgb = self.rgb_faces[idx]\n        fft = self.fft_images[idx]\n        motion = self.motion_images[idx]\n        label = self.labels[idx]\n        if self.transform:\n            rgb = self.transform(rgb)\n            fft = self.transform(fft)\n            motion = self.transform(motion)\n        return rgb, fft, motion, torch.tensor(label, dtype=torch.float32)\n\ntest_dataset = MultiStreamDeepfakeDataset(X_faces_tensor, fft_tensors, motion_tensors, y_labels)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T21:02:13.958459Z","iopub.execute_input":"2025-04-19T21:02:13.958955Z","iopub.status.idle":"2025-04-19T21:02:13.965279Z","shell.execute_reply.started":"2025-04-19T21:02:13.958931Z","shell.execute_reply":"2025-04-19T21:02:13.964480Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n\nmodel.eval()\nall_preds, all_labels, all_probs = [], [], []\n\nwith torch.no_grad():\n    for rgb, fft, motion, labels in test_loader:\n        rgb, fft, motion = rgb.to(device), fft.to(device), motion.to(device)\n        outputs = model(rgb, fft, motion).squeeze().cpu()\n        probs = outputs.numpy()\n        preds = (outputs > 0.5).long()\n        all_preds.extend(preds.tolist())\n        all_labels.extend(labels.tolist())\n        all_probs.extend(probs.tolist())\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(all_labels, all_preds, target_names=[\"Real\", \"Fake\"]))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(all_labels, all_preds))\nprint(\"ROC AUC Score:\", roc_auc_score(all_labels, all_probs))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T21:30:02.546761Z","iopub.execute_input":"2025-04-19T21:30:02.547137Z","iopub.status.idle":"2025-04-19T21:30:03.065502Z","shell.execute_reply.started":"2025-04-19T21:30:02.547117Z","shell.execute_reply":"2025-04-19T21:30:03.064773Z"}},"outputs":[{"name":"stdout","text":"\nClassification Report:\n              precision    recall  f1-score   support\n\n        Real       0.50      0.93      0.65        15\n        Fake       0.50      0.07      0.12        15\n\n    accuracy                           0.50        30\n   macro avg       0.50      0.50      0.38        30\nweighted avg       0.50      0.50      0.38        30\n\nConfusion Matrix:\n [[14  1]\n [14  1]]\nROC AUC Score: 0.5333333333333333\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"\n# Constants\nCUSTOM_REAL_PATH = \"/kaggle/input/faceforensics/FF++/real\"\nCUSTOM_FAKE_PATH = \"/kaggle/input/faceforensics/FF++/fake\"\nNUM_FRAMES = 5\nmtcnn = MTCNN(image_size=224, margin=0, device=device)\n\n# Face extraction function\ndef extract_faces_from_videos(video_dir, label, num_frames=5):\n    face_list = []\n    label_list = []\n    \n    for video_file in os.listdir(video_dir):\n        video_path = os.path.join(video_dir, video_file)\n        cap = cv2.VideoCapture(video_path)\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        frame_idxs = np.linspace(0, total_frames-1, num_frames, dtype=int)\n\n        for idx in frame_idxs:\n            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n            success, frame = cap.read()\n            if not success: continue\n\n            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            face = mtcnn(rgb)\n            if face is not None:\n                face_list.append(face)\n                label_list.append(label)\n        cap.release()\n    \n    return face_list, label_list\n\n# Load custom dataset\nreal_faces, real_labels = extract_faces_from_videos(CUSTOM_REAL_PATH, 0, NUM_FRAMES)\nfake_faces, fake_labels = extract_faces_from_videos(CUSTOM_FAKE_PATH, 1, NUM_FRAMES)\n\nall_faces = real_faces + fake_faces\nall_labels = real_labels + fake_labels\n\n# Prepare tensors\ndef prepare_fft(tensor):\n    gray = tensor.mean(dim=0, keepdim=True)\n    fft = torch.fft.fft2(gray)\n    fft_mag = torch.abs(fft)\n    fft_norm = (fft_mag - fft_mag.min()) / (fft_mag.max() - fft_mag.min() + 1e-8)\n    return fft_norm.expand(3, -1, -1)\n\ndef prepare_motion(current, previous):\n    return torch.abs(current - previous)\n\nface_tensors = [\n    torch.nn.functional.interpolate(f.unsqueeze(0).float(), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n    for f in all_faces\n]\n\nfft_tensors = [prepare_fft(face) for face in face_tensors]\nmotion_tensors = [\n    prepare_motion(face_tensors[i], face_tensors[i - 1]) if i > 0 else torch.zeros_like(face_tensors[i])\n    for i in range(len(face_tensors))\n]\n\n# Inference\nall_preds = []\nall_probs = []\n\nwith torch.no_grad():\n    for i in range(len(face_tensors)):\n        rgb = face_tensors[i].unsqueeze(0).to(device)\n        fft = fft_tensors[i].unsqueeze(0).to(device)\n        motion = motion_tensors[i].unsqueeze(0).to(device)\n\n        output = model(rgb, fft, motion).item()\n        pred = int(output > 0.5)\n        \n        all_preds.append(pred)\n        all_probs.append(output)\n\n# Evaluation\nprint(\"Classification Report:\")\nprint(classification_report(all_labels, all_preds, target_names=[\"Real\", \"Fake\"]))\n\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(all_labels, all_preds))\n\nprint(\"\\nROC AUC Score:\")\nprint(roc_auc_score(all_labels, all_probs))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T21:31:16.176598Z","iopub.execute_input":"2025-04-19T21:31:16.177300Z","iopub.status.idle":"2025-04-19T21:49:45.541247Z","shell.execute_reply.started":"2025-04-19T21:31:16.177277Z","shell.execute_reply":"2025-04-19T21:49:45.540345Z"}},"outputs":[{"name":"stdout","text":"Classification Report:\n              precision    recall  f1-score   support\n\n        Real       0.94      0.97      0.95       925\n        Fake       0.97      0.94      0.95       985\n\n    accuracy                           0.95      1910\n   macro avg       0.95      0.95      0.95      1910\nweighted avg       0.95      0.95      0.95      1910\n\n\nConfusion Matrix:\n[[893  32]\n [ 59 926]]\n\nROC AUC Score:\n0.9863174646727947\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}